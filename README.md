# Domain Wall Image Generation and GAN Training

The challenges of applying machine learning models to electron microscopy images lie in two fundamental limitations:  

1. **Access to representative images**:      
To train  a dataset with machine learning, one requires a large number of images, although it depends on the problem and the complexity of the images. Capturing lattices that reveal specific features â€” such as zone axes or atomic arrangements â€” often requires computational methods to build unit cells. This involves generating unit cells, constructing supercells, and performing image simulations. Each step can be computationally demanding, although GPU/CPU-powered workstations make the workflow more practical. 

2. **Class imbalance in datasets**:    
In sub-Ã…ngstrÃ¶m microscopy like STEM (Scanning Transmission Electron Microscopy), it is common to have many images of some features but very few of others in a class of materials. The time and cost of sample preparation and instrument use often prevent the collection of balanced datasets. Even with large numbers of images, certain rare structural features may be underrepresented.  

This project showcases a solution to these challenges in the context of **deep learning for atomic-resolution microscopy**. As a testbed, we focus on **hexagonal rare-earth manganites**, which exhibit complex domain walls and require imaging at resolutions below 0.1 nm on probe-corrected STEM machines. Hexagonal rare-earth manganites (h-REMnOâ‚ƒ, RE: Y, Dy, Er, Tm, Lu, Sc) are a go-to platform for studying topological ferroelectric texturesâ€”neutral/charged walls, interlocked walls, and vortexâ€“antivortex networks that emerge via Kibbleâ€“Zurekâ€“type symmetry breaking at the trimerization/ferroelectric transition. Aberation-corrected microcopy has been a promising method of imaging atomically sharp ferroelectric walls and their switching.

---

## ðŸŽ¯ Project Goals

This repository provides a **two-stage pipeline**:

1. **Physics-based image generation**  
   Synthetic STEM-like images of hexagonal REMnOâ‚ƒ domain walls are generated using crystallographic models, Y-ion displacements, and augmentations. While the functions support multiple wall types (type A, type B, type C, type D), the current focus is on three         configurations:  
   - **UP** (upward polarization)  
   - **DN** (downward polarization)  
   - **typeC-DW** (switching walls, i.e. any UP â†” DN transition)  

   Image generation leverages **pymatgen** (https://pymatgen.org/) for structure handling (`Structure`, `CifParser`, `CifWriter`, `SpacegroupAnalyzer`) and adds atomic displacements to simulate domain walls. The package enables the direct use of crystallographic CIF files, allowing for subsequent processing to create supercells and/or introduce lattice distortions to highlight materials-dependent features. The STEM image simulation requires supercell creation, and pymatgen does this straightforwardly.    

   **References**:  
   The project also allows different types of domain walls, so-called Type I and Type II walls, as explained in the following references.

   - Lei Tian *et al.*, *Direct observation of interlocked domain walls and topological four-state vortex-like domain patterns in multiferroic YMnOâ‚ƒ*, Appl. Phys. Lett. **106**, 112903 (2015). [doi:10.1063/1.4915259](https://doi.org/10.1063/1.4915259)  
   - Q. Zhang *et al.*, *Direct Observation of Multiferroic Vortex Domains in YMnOâ‚ƒ*, Sci. Rep. **3**, 2741 (2013). [doi:10.1038/srep02741](https://doi.org/10.1038/srep02741)  

2. **GAN training on Mixed Datasets**  
   To bridge the gap between **idealized simulations** and **real atomic-resolution images**, this project implements conditional GANs (Pix2Pix and Pix2Pix augmented with WallAttention). The networks are trained to translate **displacement-field maps** into STEM-like images, capturing both the global lattice symmetry and the local disorder at domain walls. Pix2Pix serves as the robust baseline for paired image-to-image translation, while the WallAttention variant introduces spatial focus on the wall regionsâ€”critical for reproducing the sharp, heterogeneous structures of domain walls.

A distinctive feature of this pipeline is the ability to train on hybrid datasets, combining physics-generated images (from crystallographic models with controlled displacements and augmentations) with real STEM acquisitions. This mixed-data strategy supports transfer learning, enabling the model to inherit physical priors from synthetic images while fine-tuning on experimentally acquired data. The result is a generator that not only produces visually realistic STEM-like images but also respects the atomic-scale physics encoded in real measurements.

---

### ðŸŽ› Why FiLM and Attention?

- **FiLM (Feature-wise Linear Modulation):**  
  Applies per-channel scale and shift to intermediate feature maps, injecting domain labels (UP, DN, typeC) directly into the generator.  
  Ideal for **uniform domains (UP/DN) or simple switching**, where the condition is global and consistent across the whole image.  

- **Attention (WallAttention module):**  
  Provides a **pixel-wise spatial mask** that highlights domain wall regions, guiding the generator to focus on structurally complex areas, in this case, atomically sharp walls.  
  Essential for **typeC domain walls**, where heterogeneous UP/DN mixtures require learning *where* the polarity changes, not just *what* the label is.  

ðŸ‘‰ Together, **FiLM + Attention** combine global conditioning with spatial selectivity â€” enabling realistic synthesis of both simple single-axis domains and complex multi-state walls.

---

## ðŸ“‚ Project Structure

The project is organized to separate **physics-based image generation** from **GAN learning**.

```
domain_wall_generation/
â”œâ”€â”€ physics/                         # physics-based STEM image generation
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ domain_walls.py          # generate_type_c_dw, generate_tail_to_tail_domain_wall, etc.
â”‚   â”‚   â”œâ”€â”€ y_ions.py                # get_y_ions
â”‚   â”‚   â”œâ”€â”€ displacement.py          # generate_displacement_map
â”‚   â”‚   â””â”€â”€ utils.py                 # render_stem_image, show_stem_image, apply_image_augmentation
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ main.py                  # simple dataset generator
â”‚   â”‚   â”œâ”€â”€ main_parallel.py         # parallel dataset generation
â”‚   â”‚   â”œâ”€â”€ single_image_plot.py     # visualize one augmented image
â”‚   â”‚   â”œâ”€â”€ generate_single_image.py # type B domain wall example
â”‚   â”‚   â””â”€â”€ displacement_map.py      # batch displacement map builder
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ label_maps.py            # label mappings

â”œâ”€â”€ gan/                             # GAN models, training, and evaluation
â”‚   â”œâ”€â”€ ml_utils/
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ transforms.py
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â”œâ”€â”€ visualization.py
â”‚   â”‚   â””â”€â”€ augmentation.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ discriminator.py
â”‚   â”‚   â”œâ”€â”€ film.py
â”‚   â”‚   â”œâ”€â”€ unet_blocks.py
â”‚   â”‚   â”œâ”€â”€ unified_router.py
â”‚   â”‚   â””â”€â”€ base_generators/
â”‚   â”‚       â”œâ”€â”€ sharp_pix2pix.py
â”‚   â”‚       â””â”€â”€ wall_attention_generator.py
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ train_pix2pix.py
â”‚   â”‚   â””â”€â”€ train_router.py
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ plotting.py
â”‚   â”‚   â””â”€â”€ image_generation.py
â”‚   â””â”€â”€ configs/
â”‚       â”œâ”€â”€ train_defaults.yaml
â”‚       â””â”€â”€ labels.yaml

â”œâ”€â”€ scripts/                         # command-line entrypoints
â”‚   â”œâ”€â”€ build_dataset.py
â”‚   â”œâ”€â”€ train_gan.py
â”‚   â””â”€â”€ generate_with_gan.py

â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ GAN_synthesis_notebook.ipynb

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cif/
â”‚   â”œâ”€â”€ generated_images/
â”‚   â””â”€â”€ generated_disp/

â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ figures/

â”œâ”€â”€ tests/
â”‚   â””â”€â”€ __init__.py

â””â”€â”€ README.md
```

---

## ðŸ§  Key Features

- ðŸ§ª Physics-based domain wall generation with crystallographic displacements  
- ðŸ” Augmentations: salt & pepper, drift, atomic plane distortions  
- ðŸ¤– GANs: Pix2Pix and WallAttention variants  
- ðŸ“Š Metrics: SSIM, loss component tracking, epoch plots  
- ðŸ–¼ï¸ Visualization: stochastic variants, attention overlays  
- âš¡ Reproducibility: includes tests/ and ready-to-run Jupyter notebook for Kaggle (notebooks/GAN_synthesis_notebook.ipynb).
- ðŸ§© Modular design: physics generation, GAN models, and training separated for easier extension or substitution.
---

## ðŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/domain_wall_generation.git
cd domain_wall_generation
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```
ðŸ’¡ Recommended environment: Python 3.9+, PyTorch >= 1.12, GPU with CUDA support. Tested with NVIDIA RTX GPUs. Make sure pymatgen, kornia, and pytorch-msssim are installed properly for structure handling and loss calculations.

---

## ðŸ§ª Stage 1: Physics-based Dataset Generation

```bash
PYTHONPATH=. python scripts/build_dataset.py   --cif data/ymno3_unpolar.cif   --out outputs/stem_domain   --make-disp
```

- Images â†’ `outputs/stem_domain/`  
- Displacement maps â†’ `outputs/stem_domain_disp/`  

Tip: try scripts/single_image_plot.py first to quickly visualize one augmented example before launching a full dataset build.

---

## ðŸ¤– Stage 2: Train GAN

```bash
PYTHONPATH=. python scripts/train_gan.py   --disp-root outputs/stem_domain_disp   --image-root outputs/stem_domain   --classes '["typeC-DW"]'   --save-dir outputs/gan_runs/typec   --epochs 30
```
Supports FiLM-only (sharp_pix2pix) or FiLM+Attention (wall_attention_generator) via --model flag.

---

## ðŸŽ¨ Stage 3: Generate with Trained GAN

```bash
PYTHONPATH=. python scripts/generate_with_gan.py   --checkpoint outputs/gan_runs/typec/generator.pth   --model attention   --classes '["typeC-DW"]'   --disp-root outputs/stem_domain_disp   --image-root outputs/stem_domain   --save-dir outputs/gan_infer/typec
```
You can also explore stochastic outputs by varying latent z vectors, visualized in eval/image_generation.py.

---

## ðŸ“Š Outputs

- `outputs/stem_domain/` â€” synthetic STEM images  
- `outputs/stem_domain_disp/` â€” displacement maps  
- `outputs/gan_runs/` â€” GAN checkpoints + training logs  
- `outputs/gan_infer/` â€” GAN-generated images  
- `outputs/figures/` â€” loss curves, SSIM distributions, attention overlays.

---

## ðŸ“œ License

Distributed under the MIT License. See `LICENSE` for details.
If you use this code in academic work, please cite the references in the Physics-based section above.

---

## ðŸ™‹ Contact

Questions, feedback, or collaborations?  
Open a GitHub issue or contact: [alibaghizade@gmail.com](mailto:alibaghizade@gmail.com)  
